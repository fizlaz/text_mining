{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# QUESTION ONE\n",
    "import numpy as np\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk import PorterStemmer\n",
    "from collections import Counter\n",
    "\"\"\"\n",
    "This is a class sherlock. \n",
    "Notice how it is defined with the keyword `class` and a name that begins with a capital letter\n",
    "\"\"\"\n",
    "class Document():\n",
    "    \n",
    "    \"\"\" The Doc class rpresents a class of individul documents\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, speech_year, speech_pres, speech_text):\n",
    "        \"\"\"\n",
    "        The __init__ method is called everytime an object is instantiated.\n",
    "        This is where you will define all the properties of the object that it must have\n",
    "        when it is `born`.\n",
    "        \"\"\"\n",
    "        \n",
    "        #These are data members\n",
    "        self.year = speech_year\n",
    "        self.pres = speech_pres\n",
    "        self.text = speech_text.lower()\n",
    "        self.tokens = np.array(wordpunct_tokenize(self.text))\n",
    "        \n",
    "        \n",
    "        \n",
    "    def token_clean(self,length):\n",
    "\n",
    "        \"\"\" \n",
    "        description: strip out non-alpha tokens and tokens of length > 'length'\n",
    "        input: length: cut off length \n",
    "        \"\"\"\n",
    "\n",
    "        self.tokens = np.array([t for t in self.tokens if (t.isalpha() and len(t) > length)])\n",
    "\n",
    "\n",
    "    def stopword_remove(self, stopwords):\n",
    "\n",
    "        \"\"\"\n",
    "        description: Remove stopwords from tokens.\n",
    "        input: stopwords: a suitable list of stopwords\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        self.tokens = np.array([t for t in self.tokens if t not in stopwords])\n",
    "\n",
    "\n",
    "    def stem(self):\n",
    "\n",
    "        \"\"\"\n",
    "        description: Stem tokens with Porter Stemmer.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.tokens = np.array([PorterStemmer().stem(t) for t in self.tokens])\n",
    "        \n",
    "\n",
    "        \n",
    "    def demo_self():\n",
    "        print 'this will error out'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import codecs\n",
    "import nltk\n",
    "import math\n",
    "import re\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk import PorterStemmer\n",
    "from __future__ import division\n",
    "from collections import Counter\n",
    "import copy\n",
    "\n",
    "class Corpus():\n",
    "    \n",
    "    \"\"\" \n",
    "    The Corpus class represents a document collection\n",
    "     \n",
    "    \"\"\"\n",
    "    def __init__(self, doc_data, stopword_file, clean_length):\n",
    "        \"\"\"\n",
    "        Notice that the __init__ method is invoked everytime an object of the class\n",
    "        is instantiated\n",
    "        \"\"\"\n",
    "        \n",
    "        #Initialise documents by invoking the appropriate class\n",
    "        self.docs = [Document(doc[0], doc[1], doc[2]) for doc in doc_data] \n",
    "        \n",
    "        self.N = len(self.docs)\n",
    "        self.clean_length = clean_length\n",
    "        \n",
    "        #get a list of stopwords\n",
    "        self.create_stopwords(stopword_file, clean_length)\n",
    "        \n",
    "        #stopword removal, token cleaning and stemming to docs\n",
    "        self.clean_docs(2)\n",
    "        \n",
    "        #create vocabulary\n",
    "        self.corpus_tokens()\n",
    "        \n",
    "        self.document_term_matrix1()\n",
    "        \n",
    "        self.tf_idf1()\n",
    "        \n",
    "        self.tf_idf2()\n",
    "        \n",
    "    def clean_docs(self, length):\n",
    "        \"\"\" \n",
    "        Applies stopword removal, token cleaning and stemming to docs\n",
    "        \"\"\"\n",
    "        for doc in self.docs:\n",
    "            doc.token_clean(length)\n",
    "            doc.stopword_remove(self.stopwords)\n",
    "            doc.stem()        \n",
    "    \n",
    "    def create_stopwords(self, stopword_file, length):\n",
    "        \"\"\"\n",
    "        description: parses a file of stowords, removes words of length 'length' and \n",
    "        stems it\n",
    "        input: length: cutoff length for words\n",
    "               stopword_file: stopwords file to parse\n",
    "        \"\"\"\n",
    "        \n",
    "        with codecs.open(stopword_file,'r','utf-8') as f: raw = f.read()\n",
    "        \n",
    "        self.stopwords = (np.array([PorterStemmer().stem(word) \n",
    "                                    for word in list(raw.splitlines()) if len(word) > length]))\n",
    "        \n",
    "     \n",
    "    def corpus_tokens(self):\n",
    "        \"\"\"\n",
    "        description: create a set of all all tokens or in other words a vocabulary\n",
    "        \"\"\"\n",
    "        \n",
    "        #initialise an empty set\n",
    "        self.token_set = set()\n",
    "        for doc in self.docs:\n",
    "            self.token_set = self.token_set.union(doc.tokens) \n",
    "            \n",
    "    def document_term_matrix(self):\n",
    "        \"\"\"\n",
    "        description: create a matrix listing the number of times each vocab term appears in each doc\n",
    "        \"\"\"\n",
    "        \n",
    "        # get v, the number of tokens\n",
    "        v = len(self.token_set)\n",
    "        \n",
    "        \n",
    "        doc_term_matrix = []\n",
    "        for doc in self.docs: \n",
    "\n",
    "            # create an empty dictionary of words, add the counts\n",
    "            wordlist_dict = {}\n",
    "            word_list = doc.tokens\n",
    "            for i in self.token_set:\n",
    "                wordlist_dict[i] = 0.0\n",
    "            for word in word_list:\n",
    "                wordlist_dict[word] += 1\n",
    "            # for each doc, we append its list of word counts to the doc term matrix    \n",
    "            doc_term_matrix.append(wordlist_dict.values())\n",
    "        \n",
    "        # turn the resulting list into a Dxv matrix\n",
    "        self.doc_term_matrix = np.array(doc_term_matrix)\n",
    "        self.doc_term_matrix = self.doc_term_matrix.reshape((self.N, v))\n",
    "        \n",
    "        \n",
    "    def tf_idf(self):\n",
    "        # call document_term_matrix\n",
    "        self.document_term_matrix()\n",
    "        \n",
    "        tf = np.copy(self.doc_term_matrix)\n",
    "        df = [0] * len(self.token_set)\n",
    "        \n",
    "        # create the tf and df values\n",
    "        for i in range(len(tf)):\n",
    "            for j in range(len(tf[i])):\n",
    "                if tf[i][j] > 0:\n",
    "                    df[j] += 1\n",
    "                    tf[i][j] = 1 + math.log(tf[i][j])\n",
    "        \n",
    "        for j in range(len(df)):\n",
    "            df[j] = math.log(self.N/df[j])\n",
    "        \n",
    "        # put it together into a tf-idf matrix\n",
    "        for i in range(len(tf)):\n",
    "            for j in range(len(tf[i])):\n",
    "                tf[i][j] = tf[i][j] * df[j]\n",
    "         \n",
    "\n",
    "        self.tfidf = tf\n",
    "    \n",
    "    def dict_rank(self, dictionary, representation, n):\n",
    "        \n",
    "        self.tf_idf()\n",
    "\n",
    "        # Choose whether to rely on the doc term matrix of the tf-idf matrix\n",
    "        if representation == \"doc-term\":\n",
    "            compare_docs = copy.copy(self.doc_term_matrix)\n",
    "        elif representation == \"tf-idf\":     \n",
    "            compare_docs = copy.copy(self.tfidf)\n",
    "\n",
    "        # Prepare the variables for use later\n",
    "        docs = copy.copy(self.docs)\n",
    "        doclist = []\n",
    "        weights = [0] * self.N\n",
    "        j = 0\n",
    "        \n",
    "        # Collect the weights of each document\n",
    "        for doc in docs:\n",
    "            i = 0\n",
    "            for token in iter(self.token_set):\n",
    "                if token in dictionary:\n",
    "                    weights[j] += compare_docs[j][i]\n",
    "                i += 1\n",
    "            j += 1\n",
    "        \n",
    "        # take the documents with the n largest weights\n",
    "        for i in range(n):\n",
    "            whichmax = weights.index(max(weights))\n",
    "            docmax = docs[whichmax]\n",
    "            doclist.append(docmax)\n",
    "            weights.remove(max(weights))\n",
    "            docs.remove(docmax)\n",
    "        \n",
    "        # Add it to self\n",
    "        self.dictrank = doclist \n",
    "\n",
    "        \n",
    "    ################## dom's version      \n",
    "    def document_term_matrix1(self):\n",
    "        import pandas as pd\n",
    "        srted = sorted(self.token_set)\n",
    "        \"\"\" return a D by V array of frequency counts \"\"\"\n",
    "        self.array = pd.DataFrame(columns=srted)\n",
    "        for doc in self.docs:\n",
    "            ls = doc.tokens\n",
    "            unique, counts = np.unique(ls, return_counts=True)\n",
    "            dc = dict(zip(unique,counts))\n",
    "            big = {key: 0 for key in srted}\n",
    "            big.update(dc)\n",
    "            row = big.values()\n",
    "            self.array.loc[doc]=row\n",
    "            \n",
    "        self.array.index = range(1,len(self.docs)+1)\n",
    "        \n",
    "    \"\"\" this is using double normalization 0.5 instead of log normalization   \n",
    "    def tf_idf1(self):\n",
    "        num_per_doc = self.array.astype(bool).sum(axis=0)\n",
    "        \n",
    "        idf_all = np.log(self.N/num_per_doc)\n",
    "        \n",
    "        import pandas as pd\n",
    "        srted = sorted(self.token_set)\n",
    "        self.tfidf1 = pd.DataFrame(columns=srted)\n",
    "\n",
    "        for i in (range(self.N+1)[1:]):\n",
    "            self.tfidf1.loc[i] = (self.array.xs(i)/(2*max(self.array.xs(i)))+0.5)*idf_all\n",
    "    \"\"\"\n",
    "    \n",
    "    def tf_idf2(self):\n",
    "        num_per_doc = self.array.astype(bool).sum(axis=0)\n",
    "        \n",
    "        idf_all = np.log(self.N/num_per_doc)\n",
    "        \n",
    "        import pandas as pd\n",
    "        srted = sorted(self.token_set)\n",
    "        self.tfidf2 = pd.DataFrame(columns=srted)\n",
    "        \n",
    "        def log_norm(x):\n",
    "            if x==0:\n",
    "                return 0\n",
    "            else:\n",
    "                return(1+np.log(x))\n",
    "        \n",
    "        array1 = self.array.applymap(log_norm)\n",
    "\n",
    "        for i in (range(self.N+1)[1:]):\n",
    "            self.tfidf2.loc[i] = array1.xs(i)*idf_all\n",
    "\n",
    "                     \n",
    "    def dict_rank1(self, dictionary, representation, n=10):\n",
    "        import pandas as pd\n",
    "        srted = sorted(self.token_set)\n",
    "        dict_intersection = list(set(srted) & set(dictionary))\n",
    "        if representation == \"doc-term\":\n",
    "            rowsums = np.sum(self.array[dict_intersection],axis=1)\n",
    "            b=pd.DataFrame(rowsums, columns=[\"frequency\"])\n",
    "            b=b.sort_values(by=\"frequency\",ascending=0)\n",
    "            self.dictrank1 = b[:n]\n",
    "        elif representation == \"tf-idf\":     \n",
    "            rowsums = np.sum(self.tfidf1[dict_intersection],axis=1)\n",
    "            b=pd.DataFrame(rowsums, columns=[\"frequency\"])\n",
    "            b=b.sort_values(by=\"frequency\",ascending=0)\n",
    "            self.dictrank1 = b[:n]\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def parse_text(textraw, regex):\n",
    "    \"\"\"takes raw string and performs two operations\n",
    "    1. Breaks text into a list of speech, president and speech\n",
    "    2. breaks speech into paragraphs\n",
    "    \"\"\"\n",
    "    prs_yr_spch_reg = re.compile(regex, re.MULTILINE|re.DOTALL)\n",
    "    \n",
    "    #Each tuple contains the year, last ane of the president and the speech text\n",
    "    prs_yr_spch = prs_yr_spch_reg.findall(textraw)\n",
    "    \n",
    "    #convert immutabe tuple to mutable list\n",
    "    prs_yr_spch = [list(tup) for tup in prs_yr_spch]\n",
    "    \n",
    "    for i in range(len(prs_yr_spch)):\n",
    "        prs_yr_spch[i][2] = prs_yr_spch[i][2].replace('\\n', '')\n",
    "    \n",
    "    #sort\n",
    "    prs_yr_spch.sort()\n",
    "    \n",
    "    return(prs_yr_spch)        \n",
    "\n",
    "text = open('../week0/sou_all.txt', 'r').read()\n",
    "regex = \"_(\\d{4}).*?_[a-zA-Z]+.*?_[a-zA-Z]+.*?_([a-zA-Z]+)_\\*+(\\\\n{2}.*?)\\\\n{3}\"\n",
    "pres_speech_list = parse_text(text, regex)\n",
    "\n",
    "\n",
    "#Instantite the corpus class\n",
    "corpus = Corpus(pres_speech_list, '../data/stopwords.txt', 2)\n",
    "#print corpus.docs[0].text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/dist-packages/pandas/io/parsers.py:1070: DtypeWarning: Columns (63,108,109,110,176) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = self._reader.read(nrows)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[127.0,\n",
       " 98.0,\n",
       " 184.0,\n",
       " 176.0,\n",
       " 167.0,\n",
       " 260.0,\n",
       " 200.0,\n",
       " 263.0,\n",
       " 181.0,\n",
       " 223.0,\n",
       " 146.0,\n",
       " 131.0,\n",
       " 282.0,\n",
       " 209.0,\n",
       " 200.0,\n",
       " 164.0,\n",
       " 254.0,\n",
       " 287.0,\n",
       " 214.0,\n",
       " 274.0,\n",
       " 172.0,\n",
       " 237.0,\n",
       " 216.0,\n",
       " 310.0,\n",
       " 302.0,\n",
       " 214.0,\n",
       " 285.0,\n",
       " 307.0,\n",
       " 400.0,\n",
       " 432.0,\n",
       " 438.0,\n",
       " 358.0,\n",
       " 517.0,\n",
       " 476.0,\n",
       " 607.0,\n",
       " 852.0,\n",
       " 784.0,\n",
       " 717.0,\n",
       " 581.0,\n",
       " 703.0,\n",
       " 980.0,\n",
       " 1447.0,\n",
       " 610.0,\n",
       " 719.0,\n",
       " 720.0,\n",
       " 1197.0,\n",
       " 984.0,\n",
       " 1134.0,\n",
       " 1027.0,\n",
       " 1053.0,\n",
       " 1181.0,\n",
       " 873.0,\n",
       " 783.0,\n",
       " 787.0,\n",
       " 695.0,\n",
       " 848.0,\n",
       " 1483.0,\n",
       " 1533.0,\n",
       " 1489.0,\n",
       " 1894.0,\n",
       " 716.0,\n",
       " 768.0,\n",
       " 1312.0,\n",
       " 973.0,\n",
       " 927.0,\n",
       " 1003.0,\n",
       " 1140.0,\n",
       " 1030.0,\n",
       " 1290.0,\n",
       " 1600.0,\n",
       " 1283.0,\n",
       " 1362.0,\n",
       " 634.0,\n",
       " 778.0,\n",
       " 581.0,\n",
       " 576.0,\n",
       " 832.0,\n",
       " 671.0,\n",
       " 1115.0,\n",
       " 903.0,\n",
       " 679.0,\n",
       " 889.0,\n",
       " 601.0,\n",
       " 952.0,\n",
       " 1012.0,\n",
       " 906.0,\n",
       " 1106.0,\n",
       " 631.0,\n",
       " 983.0,\n",
       " 770.0,\n",
       " 1108.0,\n",
       " 1335.0,\n",
       " 1275.0,\n",
       " 1026.0,\n",
       " 829.0,\n",
       " 860.0,\n",
       " 1915.0,\n",
       " 1468.0,\n",
       " 483.0,\n",
       " 1252.0,\n",
       " 1222.0,\n",
       " 1041.0,\n",
       " 1558.0,\n",
       " 1358.0,\n",
       " 1283.0,\n",
       " 1655.0,\n",
       " 1392.0,\n",
       " 1477.0,\n",
       " 1189.0,\n",
       " 1947.0,\n",
       " 2207.0,\n",
       " 1880.0,\n",
       " 1935.0,\n",
       " 906.0,\n",
       " 1558.0,\n",
       " 1705.0,\n",
       " 2455.0,\n",
       " 2231.0,\n",
       " 2573.0,\n",
       " 1912.0,\n",
       " 1411.0,\n",
       " 2821.0,\n",
       " 2403.0,\n",
       " 2668.0,\n",
       " 286.0,\n",
       " 394.0,\n",
       " 703.0,\n",
       " 141.0,\n",
       " 303.0,\n",
       " 465.0,\n",
       " 445.0,\n",
       " 265.0,\n",
       " 476.0,\n",
       " 468.0,\n",
       " 669.0,\n",
       " 647.0,\n",
       " 1039.0,\n",
       " 938.0,\n",
       " 849.0,\n",
       " 841.0,\n",
       " 1054.0,\n",
       " 420.0,\n",
       " 509.0,\n",
       " 389.0,\n",
       " 225.0,\n",
       " 344.0,\n",
       " 356.0,\n",
       " 277.0,\n",
       " 423.0,\n",
       " 394.0,\n",
       " 287.0,\n",
       " 312.0,\n",
       " 344.0,\n",
       " 463.0,\n",
       " 356.0,\n",
       " 842.0,\n",
       " 286.0,\n",
       " 2854.0,\n",
       " 566.0,\n",
       " 477.0,\n",
       " 359.0,\n",
       " 527.0,\n",
       " 367.0,\n",
       " 477.0,\n",
       " 713.0,\n",
       " 912.0,\n",
       " 666.0,\n",
       " 736.0,\n",
       " 819.0,\n",
       " 78.0,\n",
       " 401.0,\n",
       " 515.0,\n",
       " 531.0,\n",
       " 561.0,\n",
       " 629.0,\n",
       " 557.0,\n",
       " 701.0,\n",
       " 551.0,\n",
       " 306.0,\n",
       " 293.0,\n",
       " 527.0,\n",
       " 705.0,\n",
       " 517.0,\n",
       " 375.0,\n",
       " 451.0,\n",
       " 506.0,\n",
       " 359.0,\n",
       " 1623.0,\n",
       " 168.0,\n",
       " 432.0,\n",
       " 507.0,\n",
       " 2125.0,\n",
       " 387.0,\n",
       " 503.0,\n",
       " 478.0,\n",
       " 430.0,\n",
       " 1197.0,\n",
       " 324.0,\n",
       " 2052.0,\n",
       " 313.0,\n",
       " 3150.0,\n",
       " 3154.0,\n",
       " 422.0,\n",
       " 515.0,\n",
       " 568.0,\n",
       " 534.0,\n",
       " 425.0,\n",
       " 395.0,\n",
       " 363.0,\n",
       " 566.0,\n",
       " 473.0,\n",
       " 395.0,\n",
       " 412.0,\n",
       " 459.0,\n",
       " 617.0,\n",
       " 670.0,\n",
       " 785.0,\n",
       " 654.0,\n",
       " 691.0,\n",
       " 763.0,\n",
       " 760.0,\n",
       " 906.0,\n",
       " 445.0,\n",
       " 422.0,\n",
       " 496.0,\n",
       " 521.0,\n",
       " 476.0,\n",
       " 517.0,\n",
       " 569.0,\n",
       " 580.0,\n",
       " 611.0,\n",
       " 667.0,\n",
       " 648.0,\n",
       " 691.0,\n",
       " 639.0,\n",
       " 697.0]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Load in the data\n",
    "harvard = pd.read_table('../data/inquirerbasic.csv', encoding = \"utf-8\", sep = ',')\n",
    "\n",
    "### dom cleaning\n",
    "df = harvard.parse()\n",
    "\n",
    "a=df[\"Entry\"].tolist()\n",
    "\n",
    "regex = re.compile('[^a-zA-Z]')\n",
    "\n",
    "for i in range(len(a)):\n",
    "    a[i] = str(a[i])\n",
    "    \n",
    "for i in range(len(a)):\n",
    "    a[i] = regex.sub('', a[i])\n",
    "    \n",
    "for i in range(len(a)):\n",
    "    a[i] = a[i].lower()\n",
    "    \n",
    "terms = a\n",
    "###\n",
    "\n",
    "# We are going to look to see which of the speeches is most positive\n",
    "# Create a dictionary of the \n",
    "#terms = harvard['Entry']\n",
    "positive = harvard['Positiv']\n",
    "pos_terms = dict(zip(terms, positive))\n",
    "\n",
    "corpus.document_term_matrix()\n",
    "\n",
    "# Create a score based on the document term matrix\n",
    "harvard_score = [0] * len(corpus.docs)\n",
    "for doc in range(len(corpus.docs)):\n",
    "    tokencount = 0\n",
    "    for token in iter(corpus.token_set):\n",
    "        if token.upper() in pos_terms:\n",
    "            harvard_score[doc] += corpus.doc_term_matrix[doc][tokencount]\n",
    "        tokencount += 1\n",
    "harvard_score\n",
    "\n",
    "# we collect a list of decades so as to compare the speeches\n",
    "decades = []\n",
    "for doc in range(len(corpus.docs)):\n",
    "    decades.append(int(corpus.docs[doc].year) // 10)\n",
    "    \n",
    "# collect the total positivity score of each decade, and the total number of speeches for that decade\n",
    "tot_score_decade = dict(zip(set(decades), [0] * len(set(decades)) ))\n",
    "tot_num_decade = dict(zip(set(decades), [0] * len(set(decades)) ))\n",
    "# Arguably the worst code ever written\n",
    "for decade in iter(set(decades)):\n",
    "    for i in range(len(decades)):\n",
    "        if decades[i] == decade:\n",
    "            tot_score_decade[decade] += harvard_score[i]\n",
    "            tot_num_decade[decade] += 1\n",
    "            \n",
    "# Find the avg\n",
    "avg_score_decade = dict(zip(set(decades), np.array(tot_score_decade.values()) / np.array(tot_num_decade.values())))\n",
    "avg_score_decade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{179: 75.529720245783082,\n",
       " 180: 79.796968611307989,\n",
       " 181: 117.00793291445818,\n",
       " 182: 221.13446114888447,\n",
       " 183: 319.40272820869586,\n",
       " 184: 360.25412714809283,\n",
       " 185: 406.06344332756385,\n",
       " 186: 311.76881045773371,\n",
       " 187: 311.58545862814077,\n",
       " 188: 452.15421507681685,\n",
       " 189: 590.51799758266691,\n",
       " 190: 710.27910581935157,\n",
       " 191: 355.90895194198595,\n",
       " 192: 245.76912290548117,\n",
       " 193: 131.27552590543115,\n",
       " 194: 221.05955380046828,\n",
       " 195: 203.23708865967771,\n",
       " 196: 237.08213357469759,\n",
       " 197: 271.28890522217705,\n",
       " 198: 369.79087368573096,\n",
       " 199: 297.39829052601192,\n",
       " 200: 284.2533123055922,\n",
       " 201: 353.29554321530941}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We now repeat the exercise, but using the tf-idf matrix instead\n",
    "corpus.tf_idf()\n",
    "\n",
    "# Create a score based on the document term matrix\n",
    "harvard_score = [0] * len(corpus.docs)\n",
    "for doc in range(len(corpus.docs)):\n",
    "    tokencount = 0\n",
    "    for token in iter(corpus.token_set):\n",
    "        if token.upper() in pos_terms:\n",
    "            harvard_score[doc] += corpus.tfidf[doc][tokencount]\n",
    "        tokencount += 1\n",
    "harvard_score\n",
    "\n",
    "# we collect a list of decades so as to compare the speeches\n",
    "decades = []\n",
    "for doc in range(len(corpus.docs)):\n",
    "    decades.append(int(corpus.docs[doc].year) // 10)\n",
    "    \n",
    "# collect the total positivity score of each decade, and the total number of speeches for that decade\n",
    "tot_score_decade = dict(zip(set(decades), [0] * len(set(decades)) ))\n",
    "tot_num_decade = dict(zip(set(decades), [0] * len(set(decades)) ))\n",
    "# Arguably the worst code ever written\n",
    "for decade in iter(set(decades)):\n",
    "    for i in range(len(decades)):\n",
    "        if decades[i] == decade:\n",
    "            tot_score_decade[decade] += harvard_score[i]\n",
    "            tot_num_decade[decade] += 1\n",
    "            \n",
    "# Find the avg\n",
    "avg_score_decade = dict(zip(set(decades), np.array(tot_score_decade.values()) / np.array(tot_num_decade.values())))\n",
    "avg_score_decade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "afinn = dict(map(lambda (k,v): (k,int(v)), \n",
    "                     [ line.split('\\t') for line in open('../data/AFINN-111.txt') ]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-22.0,\n",
       " 10.0,\n",
       " -38.0,\n",
       " -22.0,\n",
       " -21.0,\n",
       " -10.0,\n",
       " -19.0,\n",
       " -28.0,\n",
       " -31.0,\n",
       " -11.0,\n",
       " -5.0,\n",
       " -9.0,\n",
       " -19.0,\n",
       " -42.0,\n",
       " -42.0,\n",
       " -25.0,\n",
       " -52.0,\n",
       " -22.0,\n",
       " -21.0,\n",
       " -25.0,\n",
       " -1.0,\n",
       " 11.0,\n",
       " -31.0,\n",
       " -20.0,\n",
       " -11.0,\n",
       " -13.0,\n",
       " -37.0,\n",
       " -16.0,\n",
       " -28.0,\n",
       " -33.0,\n",
       " -1.0,\n",
       " 14.0,\n",
       " -51.0,\n",
       " -14.0,\n",
       " -41.0,\n",
       " -42.0,\n",
       " -56.0,\n",
       " -14.0,\n",
       " -58.0,\n",
       " -54.0,\n",
       " -61.0,\n",
       " -121.0,\n",
       " -99.0,\n",
       " -123.0,\n",
       " -84.0,\n",
       " -114.0,\n",
       " -98.0,\n",
       " -82.0,\n",
       " -125.0,\n",
       " -112.0,\n",
       " -146.0,\n",
       " -87.0,\n",
       " -72.0,\n",
       " -94.0,\n",
       " -55.0,\n",
       " -39.0,\n",
       " -112.0,\n",
       " 10.0,\n",
       " -75.0,\n",
       " -134.0,\n",
       " -101.0,\n",
       " -39.0,\n",
       " -176.0,\n",
       " -130.0,\n",
       " -97.0,\n",
       " -144.0,\n",
       " -74.0,\n",
       " -84.0,\n",
       " 2.0,\n",
       " -186.0,\n",
       " -62.0,\n",
       " -85.0,\n",
       " -43.0,\n",
       " -107.0,\n",
       " -67.0,\n",
       " -51.0,\n",
       " -19.0,\n",
       " -75.0,\n",
       " -179.0,\n",
       " -97.0,\n",
       " -62.0,\n",
       " -47.0,\n",
       " -38.0,\n",
       " -118.0,\n",
       " -50.0,\n",
       " -95.0,\n",
       " -71.0,\n",
       " -3.0,\n",
       " -208.0,\n",
       " -161.0,\n",
       " -123.0,\n",
       " -173.0,\n",
       " -217.0,\n",
       " -136.0,\n",
       " -105.0,\n",
       " -102.0,\n",
       " -205.0,\n",
       " -175.0,\n",
       " -78.0,\n",
       " -202.0,\n",
       " -165.0,\n",
       " -85.0,\n",
       " -181.0,\n",
       " -97.0,\n",
       " -179.0,\n",
       " -237.0,\n",
       " -212.0,\n",
       " -188.0,\n",
       " -130.0,\n",
       " -168.0,\n",
       " -148.0,\n",
       " -140.0,\n",
       " -33.0,\n",
       " -88.0,\n",
       " -171.0,\n",
       " -113.0,\n",
       " -128.0,\n",
       " -51.0,\n",
       " -20.0,\n",
       " -137.0,\n",
       " -163.0,\n",
       " -287.0,\n",
       " -200.0,\n",
       " -140.0,\n",
       " -44.0,\n",
       " -53.0,\n",
       " -88.0,\n",
       " 1.0,\n",
       " -39.0,\n",
       " -37.0,\n",
       " -63.0,\n",
       " -71.0,\n",
       " -40.0,\n",
       " -49.0,\n",
       " -68.0,\n",
       " 7.0,\n",
       " -20.0,\n",
       " -100.0,\n",
       " -116.0,\n",
       " -103.0,\n",
       " -95.0,\n",
       " -53.0,\n",
       " -72.0,\n",
       " -49.0,\n",
       " -20.0,\n",
       " -41.0,\n",
       " -65.0,\n",
       " 15.0,\n",
       " -57.0,\n",
       " -49.0,\n",
       " -32.0,\n",
       " -17.0,\n",
       " -9.0,\n",
       " 35.0,\n",
       " -12.0,\n",
       " -29.0,\n",
       " -37.0,\n",
       " -253.0,\n",
       " -79.0,\n",
       " 24.0,\n",
       " -6.0,\n",
       " 3.0,\n",
       " -10.0,\n",
       " -2.0,\n",
       " -91.0,\n",
       " -94.0,\n",
       " -75.0,\n",
       " -86.0,\n",
       " -161.0,\n",
       " -4.0,\n",
       " -92.0,\n",
       " -26.0,\n",
       " -59.0,\n",
       " -65.0,\n",
       " -54.0,\n",
       " -52.0,\n",
       " -54.0,\n",
       " -6.0,\n",
       " -20.0,\n",
       " -19.0,\n",
       " -61.0,\n",
       " -17.0,\n",
       " -31.0,\n",
       " 12.0,\n",
       " -8.0,\n",
       " -34.0,\n",
       " -8.0,\n",
       " -241.0,\n",
       " -17.0,\n",
       " -55.0,\n",
       " -25.0,\n",
       " -268.0,\n",
       " -44.0,\n",
       " -57.0,\n",
       " -46.0,\n",
       " -1.0,\n",
       " -197.0,\n",
       " -26.0,\n",
       " -218.0,\n",
       " -56.0,\n",
       " -536.0,\n",
       " -383.0,\n",
       " -27.0,\n",
       " -14.0,\n",
       " -15.0,\n",
       " 7.0,\n",
       " 18.0,\n",
       " 11.0,\n",
       " 15.0,\n",
       " 10.0,\n",
       " 12.0,\n",
       " 9.0,\n",
       " -51.0,\n",
       " -1.0,\n",
       " -34.0,\n",
       " -11.0,\n",
       " 1.0,\n",
       " 15.0,\n",
       " 2.0,\n",
       " -45.0,\n",
       " -4.0,\n",
       " -1.0,\n",
       " -7.0,\n",
       " -30.0,\n",
       " -35.0,\n",
       " -21.0,\n",
       " -39.0,\n",
       " -46.0,\n",
       " -16.0,\n",
       " -18.0,\n",
       " -32.0,\n",
       " -41.0,\n",
       " 55.0,\n",
       " -31.0,\n",
       " -10.0,\n",
       " 55.0]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WE STILL HAVEN'T DONE ANYTHING ABOUT THE FACT THAT THE AFINN IS NOT TOKENISED\n",
    "\n",
    "sent_score = [0] * len(corpus.docs)\n",
    "for doc in range(len(corpus.docs)):\n",
    "    tokencount = 0\n",
    "    for token in iter(corpus.token_set):\n",
    "        try:\n",
    "            sent_score[doc] += afinn[token] * corpus.doc_term_matrix[doc][tokencount]\n",
    "        except:\n",
    "            None\n",
    "        tokencount += 1\n",
    "sent_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
