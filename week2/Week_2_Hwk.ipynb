{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PS2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD function inserted in Corpus class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# QUESTION ONE\n",
    "import numpy as np\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk import PorterStemmer\n",
    "from collections import Counter\n",
    "\"\"\"\n",
    "This is a class sherlock. \n",
    "Notice how it is defined with the keyword `class` and a name that begins with a capital letter\n",
    "\"\"\"\n",
    "class Document():\n",
    "    \n",
    "    \"\"\" The Doc class rpresents a class of individul documents\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, speech_year, speech_pres, speech_text):\n",
    "        \"\"\"\n",
    "        The __init__ method is called everytime an object is instantiated.\n",
    "        This is where you will define all the properties of the object that it must have\n",
    "        when it is `born`.\n",
    "        \"\"\"\n",
    "        \n",
    "        #These are data members\n",
    "        self.year = speech_year\n",
    "        self.pres = speech_pres\n",
    "        self.text = speech_text.lower()\n",
    "        self.tokens = np.array(wordpunct_tokenize(self.text))\n",
    "        \n",
    "        \n",
    "        \n",
    "    def token_clean(self,length):\n",
    "\n",
    "        \"\"\" \n",
    "        description: strip out non-alpha tokens and tokens of length > 'length'\n",
    "        input: length: cut off length \n",
    "        \"\"\"\n",
    "\n",
    "        self.tokens = np.array([t for t in self.tokens if (t.isalpha() and len(t) > length)])\n",
    "\n",
    "\n",
    "    def stopword_remove(self, stopwords):\n",
    "\n",
    "        \"\"\"\n",
    "        description: Remove stopwords from tokens.\n",
    "        input: stopwords: a suitable list of stopwords\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        self.tokens = np.array([t for t in self.tokens if t not in stopwords])\n",
    "\n",
    "\n",
    "    def stem(self):\n",
    "\n",
    "        \"\"\"\n",
    "        description: Stem tokens with Porter Stemmer.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.tokens = np.array([PorterStemmer().stem(t) for t in self.tokens])\n",
    "        \n",
    "\n",
    "        \n",
    "    def demo_self():\n",
    "        print 'this will error out'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import codecs\n",
    "import nltk\n",
    "import math\n",
    "import re\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk import PorterStemmer\n",
    "from __future__ import division\n",
    "from collections import Counter\n",
    "import copy\n",
    "\n",
    "class Corpus():\n",
    "    \n",
    "    \"\"\" \n",
    "    The Corpus class represents a document collection\n",
    "     \n",
    "    \"\"\"\n",
    "    def __init__(self, doc_data, stopword_file, clean_length):\n",
    "        \"\"\"\n",
    "        Notice that the __init__ method is invoked everytime an object of the class\n",
    "        is instantiated\n",
    "        \"\"\"\n",
    "        \n",
    "        #Initialise documents by invoking the appropriate class\n",
    "        self.docs = [Document(doc[0], doc[1], doc[2]) for doc in doc_data] \n",
    "        \n",
    "        self.N = len(self.docs)\n",
    "        self.clean_length = clean_length\n",
    "        \n",
    "        #get a list of stopwords\n",
    "        self.create_stopwords(stopword_file, clean_length)\n",
    "        \n",
    "        #stopword removal, token cleaning and stemming to docs\n",
    "        self.clean_docs(2)\n",
    "        \n",
    "        #create vocabulary\n",
    "        self.corpus_tokens()\n",
    "        \n",
    "        self.document_term_matrix1()\n",
    "        \n",
    "        self.tf_idf1()\n",
    "        \n",
    "    def clean_docs(self, length):\n",
    "        \"\"\" \n",
    "        Applies stopword removal, token cleaning and stemming to docs\n",
    "        \"\"\"\n",
    "        for doc in self.docs:\n",
    "            doc.token_clean(length)\n",
    "            doc.stopword_remove(self.stopwords)\n",
    "            doc.stem()        \n",
    "    \n",
    "    def create_stopwords(self, stopword_file, length):\n",
    "        \"\"\"\n",
    "        description: parses a file of stowords, removes words of length 'length' and \n",
    "        stems it\n",
    "        input: length: cutoff length for words\n",
    "               stopword_file: stopwords file to parse\n",
    "        \"\"\"\n",
    "        \n",
    "        with codecs.open(stopword_file,'r','utf-8') as f: raw = f.read()\n",
    "        \n",
    "        self.stopwords = (np.array([PorterStemmer().stem(word) \n",
    "                                    for word in list(raw.splitlines()) if len(word) > length]))\n",
    "        \n",
    "     \n",
    "    def corpus_tokens(self):\n",
    "        \"\"\"\n",
    "        description: create a set of all all tokens or in other words a vocabulary\n",
    "        \"\"\"\n",
    "        \n",
    "        #initialise an empty set\n",
    "        self.token_set = set()\n",
    "        for doc in self.docs:\n",
    "            self.token_set = self.token_set.union(doc.tokens) \n",
    "            \n",
    "    def document_term_matrix(self):\n",
    "        \"\"\"\n",
    "        description: create a matrix listing the number of times each vocab term appears in each doc\n",
    "        \"\"\"\n",
    "        \n",
    "        # get v, the number of tokens\n",
    "        v = len(self.token_set)\n",
    "        \n",
    "        \n",
    "        doc_term_matrix = []\n",
    "        for doc in self.docs: \n",
    "\n",
    "            # create an empty dictionary of words, add the counts\n",
    "            wordlist_dict = {}\n",
    "            word_list = doc.tokens\n",
    "            for i in self.token_set:\n",
    "                wordlist_dict[i] = 0.0\n",
    "            for word in word_list:\n",
    "                wordlist_dict[word] += 1\n",
    "            # for each doc, we append its list of word counts to the doc term matrix    \n",
    "            doc_term_matrix.append(wordlist_dict.values())\n",
    "        \n",
    "        # turn the resulting list into a Dxv matrix\n",
    "        self.doc_term_matrix = np.array(doc_term_matrix)\n",
    "        self.doc_term_matrix = self.doc_term_matrix.reshape((self.N, v))\n",
    "        \n",
    "        \n",
    "    def tf_idf(self):\n",
    "        # call document_term_matrix\n",
    "        self.document_term_matrix()\n",
    "        \n",
    "        tf = np.copy(self.doc_term_matrix)\n",
    "        df = [0] * len(self.token_set)\n",
    "        \n",
    "        # create the tf and df values\n",
    "        for i in range(len(tf)):\n",
    "            for j in range(len(tf[i])):\n",
    "                if tf[i][j] > 0:\n",
    "                    df[j] += 1\n",
    "                    tf[i][j] = 1 + math.log(tf[i][j])\n",
    "        \n",
    "        for j in range(len(df)):\n",
    "            df[j] = math.log(self.N/df[j])\n",
    "        \n",
    "        # put it together into a tf-idf matrix\n",
    "        for i in range(len(tf)):\n",
    "            for j in range(len(tf[i])):\n",
    "                tf[i][j] = tf[i][j] * df[j]\n",
    "         \n",
    "\n",
    "        self.tfidf = tf\n",
    "    \n",
    "    def dict_rank(self, dictionary, representation, n):\n",
    "        \n",
    "        self.tf_idf()\n",
    "\n",
    "        # Choose whether to rely on the doc term matrix of the tf-idf matrix\n",
    "        if representation == \"doc-term\":\n",
    "            compare_docs = copy.copy(self.doc_term_matrix)\n",
    "        elif representation == \"tf-idf\":     \n",
    "            compare_docs = copy.copy(self.tfidf)\n",
    "\n",
    "        # Prepare the variables for use later\n",
    "        docs = copy.copy(self.docs)\n",
    "        doclist = []\n",
    "        weights = [0] * self.N\n",
    "        j = 0\n",
    "        \n",
    "        # Collect the weights of each document\n",
    "        for doc in docs:\n",
    "            i = 0\n",
    "            for token in iter(self.token_set):\n",
    "                if token in dictionary:\n",
    "                    weights[j] += compare_docs[j][i]\n",
    "                i += 1\n",
    "            j += 1\n",
    "        \n",
    "        # take the documents with the n largest weights\n",
    "        for i in range(n):\n",
    "            whichmax = weights.index(max(weights))\n",
    "            docmax = docs[whichmax]\n",
    "            doclist.append(docmax)\n",
    "            weights.remove(max(weights))\n",
    "            docs.remove(docmax)\n",
    "        \n",
    "        # Add it to self\n",
    "        self.dictrank = doclist \n",
    "\n",
    "        \n",
    "    ################## dom's version      \n",
    "    def document_term_matrix1(self):\n",
    "        import pandas as pd\n",
    "        srted = sorted(self.token_set)\n",
    "        \"\"\" return a D by V array of frequency counts \"\"\"\n",
    "        self.array = pd.DataFrame(columns=srted)\n",
    "        for doc in self.docs:\n",
    "            ls = doc.tokens\n",
    "            unique, counts = np.unique(ls, return_counts=True)\n",
    "            dc = dict(zip(unique,counts))\n",
    "            big = {key: 0 for key in srted}\n",
    "            big.update(dc)\n",
    "            row = big.values()\n",
    "            self.array.loc[doc]=row\n",
    "            \n",
    "        self.array.index = range(1,len(self.docs)+1)\n",
    "        \n",
    "        \n",
    "    def tf_idf1(self):\n",
    "        num_per_doc = self.array.astype(bool).sum(axis=0)\n",
    "        \n",
    "        idf_all = np.log(self.N/num_per_doc)\n",
    "        \n",
    "        import pandas as pd\n",
    "        srted = sorted(self.token_set)\n",
    "        self.tfidf1 = pd.DataFrame(columns=srted)\n",
    "\n",
    "        for i in (range(self.N+1)[1:]):\n",
    "            self.tfidf1.loc[i] = (self.array.xs(i)/(2*max(self.array.xs(i)))+0.5)*idf_all\n",
    "            \n",
    "            \n",
    "            \n",
    "    def dict_rank1(self, dictionary, representation, n=10):\n",
    "        import pandas as pd\n",
    "        srted = sorted(self.token_set)\n",
    "        dict_intersection = list(set(srted) & set(dictionary))\n",
    "        if representation == \"doc-term\":\n",
    "            rowsums = np.sum(self.array[dict_intersection],axis=1)\n",
    "            b=pd.DataFrame(rowsums, columns=[\"frequency\"])\n",
    "            b=b.sort_values(by=\"frequency\",ascending=0)\n",
    "            self.dictrank1 = b[:n]\n",
    "        elif representation == \"tf-idf\":     \n",
    "            rowsums = np.sum(self.tfidf1[dict_intersection],axis=1)\n",
    "            b=pd.DataFrame(rowsums, columns=[\"frequency\"])\n",
    "            b=b.sort_values(by=\"frequency\",ascending=0)\n",
    "            self.dictrank1 = b[:n]\n",
    "            \n",
    "            \n",
    "    ################################# week 2 SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_text(textraw, regex):\n",
    "    \"\"\"takes raw string and performs two operations\n",
    "    1. Breaks text into a list of speech, president and speech\n",
    "    2. breaks speech into paragraphs\n",
    "    \"\"\"\n",
    "    prs_yr_spch_reg = re.compile(regex, re.MULTILINE|re.DOTALL)\n",
    "    \n",
    "    #Each tuple contains the year, last ane of the president and the speech text\n",
    "    prs_yr_spch = prs_yr_spch_reg.findall(textraw)\n",
    "    \n",
    "    #convert immutabe tuple to mutable list\n",
    "    prs_yr_spch = [list(tup) for tup in prs_yr_spch]\n",
    "    \n",
    "    for i in range(len(prs_yr_spch)):\n",
    "        prs_yr_spch[i][2] = prs_yr_spch[i][2].replace('\\n', '')\n",
    "    \n",
    "    #sort\n",
    "    prs_yr_spch.sort()\n",
    "    \n",
    "    return(prs_yr_spch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = open('../week0/sou_all.txt', 'r').read()\n",
    "regex = \"_(\\d{4}).*?_[a-zA-Z]+.*?_[a-zA-Z]+.*?_([a-zA-Z]+)_\\*+(\\\\n{2}.*?)\\\\n{3}\"\n",
    "pres_speech_list = parse_text(text, regex)\n",
    "\n",
    "corpus = Corpus(pres_speech_list, '../data/stopwords/stopwords.txt', 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "U, s, V = np.linalg.svd(corpus.tfidf1, full_matrices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# low rank approximation\n",
    "S = np.zeros((236,13568), dtype=complex)\n",
    "S[:100, :100] = np.diag(s[:100])\n",
    "X_hat = np.dot(U, np.dot(S, V))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "doc_magnitude = corpus.tfidf1.apply(np.linalg.norm, axis=1).values.reshape((236,1))\n",
    "\n",
    "cos_sim_mat = np.true_divide(corpus.tfidf1.dot(corpus.tfidf1.T),np.dot(doc_magnitude,doc_magnitude.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_magnitude_X_hat = np.apply_along_axis(np.linalg.norm, 1, X_hat).reshape((236,1))\n",
    "\n",
    "cos_sim_mat_X_hat = np.true_divide(X_hat.dot(X_hat.T),np.dot(doc_magnitude_X_hat,doc_magnitude_X_hat.T))\n",
    "\n",
    "import pandas as pd\n",
    "cos_sim_mat_X_hat = pd.DataFrame(cos_sim_mat_X_hat)\n",
    "cos_sim_mat_X_hat.columns = range(1,237)\n",
    "cos_sim_mat_X_hat.index = range(1,237)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1      (0.999991221327+0j)\n",
       "2                   (1+0j)\n",
       "3      (0.999997149595+0j)\n",
       "4      (0.999996538596+0j)\n",
       "5      (0.999999203248+0j)\n",
       "6      (0.999985150137+0j)\n",
       "7      (0.999990882641+0j)\n",
       "8      (0.999992475682+0j)\n",
       "9      (0.999996411529+0j)\n",
       "10     (0.999990876225+0j)\n",
       "11     (0.999985985159+0j)\n",
       "12     (0.999992354251+0j)\n",
       "13      (0.99999901833+0j)\n",
       "14     (0.999995122593+0j)\n",
       "15     (0.999997983258+0j)\n",
       "16     (0.999992510374+0j)\n",
       "17     (0.999990558812+0j)\n",
       "18     (0.999994440768+0j)\n",
       "19     (0.999982208319+0j)\n",
       "20     (0.999989886258+0j)\n",
       "21     (0.999987062402+0j)\n",
       "22     (0.999979012048+0j)\n",
       "23     (0.999985250414+0j)\n",
       "24     (0.999988026447+0j)\n",
       "25     (0.999990257768+0j)\n",
       "26     (0.999992321176+0j)\n",
       "27     (0.999995870833+0j)\n",
       "28     (0.999996330719+0j)\n",
       "29     (0.999998195158+0j)\n",
       "30     (0.999993684316+0j)\n",
       "              ...         \n",
       "207    (0.999990801958+0j)\n",
       "208    (0.999982400554+0j)\n",
       "209    (0.999986375842+0j)\n",
       "210    (0.999995137968+0j)\n",
       "211    (0.999994364741+0j)\n",
       "212      (0.9999799906+0j)\n",
       "213    (0.999988151916+0j)\n",
       "214    (0.999995895828+0j)\n",
       "215    (0.999997153077+0j)\n",
       "216    (0.999992610656+0j)\n",
       "217    (0.999991006571+0j)\n",
       "218    (0.999991937872+0j)\n",
       "219    (0.999995645236+0j)\n",
       "220     (0.99999106107+0j)\n",
       "221    (0.999989853934+0j)\n",
       "222    (0.999992845522+0j)\n",
       "223    (0.999991897017+0j)\n",
       "224    (0.999988275525+0j)\n",
       "225    (0.999978296699+0j)\n",
       "226    (0.999983379445+0j)\n",
       "227    (0.999993723568+0j)\n",
       "228     (0.99999536834+0j)\n",
       "229    (0.999990488772+0j)\n",
       "230    (0.999985361875+0j)\n",
       "231    (0.999997280006+0j)\n",
       "232    (0.999991821961+0j)\n",
       "233    (0.999990792956+0j)\n",
       "234    (0.999986708471+0j)\n",
       "235    (0.999987214156+0j)\n",
       "236    (0.999981193597+0j)\n",
       "Name: 2, dtype: complex128"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim_mat_X_hat.loc[2,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
