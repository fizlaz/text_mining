{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load in data and clean corpus\n",
    "# So, basically, what I should have done was read and clean the data in another file, then load the cleaned data\n",
    "# Problem: my ongoing incompetence.\n",
    "# So instead I just put all the re-used code (shamelessly copied from the illustrious Mr Agnihotri) in here instead.\n",
    "# YAY\n",
    "import numpy as np\n",
    "import codecs\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk import PorterStemmer\n",
    "class Document():\n",
    "    \n",
    "    \"\"\" The Doc class rpresents a class of individual documents\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, speech_year, speech_pres, speech_text):\n",
    "        self.year = speech_year\n",
    "        self.pres = speech_pres\n",
    "        self.text = speech_text.lower()\n",
    "        self.tokens = np.array(wordpunct_tokenize(self.text))\n",
    "        \n",
    "        \n",
    "        \n",
    "    def token_clean(self,length):\n",
    "\n",
    "        \"\"\" \n",
    "        description: strip out non-alpha tokens and tokens of length > 'length'\n",
    "        input: length: cut off length \n",
    "        \"\"\"\n",
    "\n",
    "        self.tokens = np.array([t for t in self.tokens if (t.isalpha() and len(t) > length)])\n",
    "\n",
    "    def stopword_remove(self, stopwords):\n",
    "\n",
    "        \"\"\"\n",
    "        description: Remove stopwords from tokens.\n",
    "        input: stopwords: a suitable list of stopwords\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        self.tokens = np.array([t for t in self.tokens if t not in stopwords])\n",
    "\n",
    "\n",
    "    def stem(self):\n",
    "\n",
    "        \"\"\"\n",
    "        description: Stem tokens with Porter Stemmer.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.tokens = np.array([PorterStemmer().stem(t) for t in self.tokens])\n",
    "class Corpus():\n",
    "    \n",
    "    \"\"\" \n",
    "    The Corpus class represents a document collection\n",
    "     \n",
    "    \"\"\"\n",
    "    def __init__(self, doc_data, stopword_file, clean_length):\n",
    "        \"\"\"\n",
    "        Notice that the __init__ method is invoked everytime an object of the class\n",
    "        is instantiated\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        #Initialise documents by invoking the appropriate class\n",
    "        self.docs = [Document(doc[0], doc[1], doc[2]) for doc in doc_data] \n",
    "        \n",
    "        self.N = len(self.docs)\n",
    "        self.clean_length = clean_length\n",
    "        \n",
    "        #get a list of stopwords\n",
    "        self.create_stopwords(stopword_file, clean_length)\n",
    "        #stopword removal, token cleaning and stemming to docs\n",
    "        self.clean_docs(2)\n",
    "        \n",
    "        #create vocabulary\n",
    "        self.corpus_tokens()\n",
    "        \n",
    "    def clean_docs(self, length):\n",
    "        \"\"\" \n",
    "        Applies stopword removal, token cleaning and stemming to docs\n",
    "        \"\"\"\n",
    "        for doc in self.docs:\n",
    "            doc.token_clean(length)\n",
    "            doc.stopword_remove(self.stopwords)\n",
    "            doc.stem()        \n",
    "    \n",
    "    def create_stopwords(self, stopword_file, length):\n",
    "        \"\"\"\n",
    "        description: parses a file of stowords, removes words of length 'length' and \n",
    "        stems it\n",
    "        input: length: cutoff length for words\n",
    "               stopword_file: stopwords file to parse\n",
    "        \"\"\"\n",
    "        \n",
    "        with codecs.open(stopword_file,'r','utf-8') as f: raw = f.read()\n",
    "        \n",
    "        self.stopwords = (np.array([PorterStemmer().stem(word) \n",
    "                                    for word in list(raw.splitlines()) if len(word) > length]))\n",
    "    def corpus_tokens(self):\n",
    "        \"\"\"\n",
    "        description: create a set of all all tokens or in other words a vocabulary\n",
    "        \"\"\"\n",
    "        \n",
    "        #initialise an empty set\n",
    "        self.token_set = set()\n",
    "        for doc in self.docs:\n",
    "            self.token_set = self.token_set.union(doc.tokens) \n",
    "            \n",
    "def parse_text(textraw, regex):\n",
    "    \"\"\"takes raw string and performs two operations\n",
    "    1. Breaks text into a list of speech, president and speech\n",
    "    2. breaks speech into paragraphs\n",
    "    \"\"\"\n",
    "    prs_yr_spch_reg = re.compile(regex, re.MULTILINE|re.DOTALL)\n",
    "    \n",
    "    #Each tuple contains the year, last ane of the president and the speech text\n",
    "    prs_yr_spch = prs_yr_spch_reg.findall(textraw)\n",
    "    \n",
    "    #convert immutabe tuple to mutable list\n",
    "    prs_yr_spch = [list(tup) for tup in prs_yr_spch]\n",
    "    \n",
    "    for i in range(len(prs_yr_spch)):\n",
    "        prs_yr_spch[i][2] = prs_yr_spch[i][2].replace('\\n', '')\n",
    "    \n",
    "    #sort\n",
    "    prs_yr_spch.sort()\n",
    "    \n",
    "    return(prs_yr_spch)\n",
    "\n",
    "text = open('./../data/pres_speech/sou_all.txt', 'r').read()\n",
    "regex = \"_(\\d{4}).*?_[a-zA-Z]+.*?_[a-zA-Z]+.*?_([a-zA-Z]+)_\\*+(\\\\n{2}.*?)\\\\n{3}\"\n",
    "pres_speech_list = parse_text(text, regex)\n",
    "#Instantite the corpus class\n",
    "corpus = Corpus(pres_speech_list, './../data/stopwords/stopwords.txt', 2)\n",
    "for doc in corpus.docs:\n",
    "    if len(doc.text) < 500:\n",
    "        print doc\n",
    "        print len(doc.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "306\n",
      "6233\n"
     ]
    }
   ],
   "source": [
    "# BUT LOOK I WROTE THIS BIT MYSELF.\n",
    "#it is 3 lines long\n",
    "corpus_forLDA = []\n",
    "for doc in corpus.docs:\n",
    "    corpus_forLDA.append(doc.tokens[0:400])\n",
    "    \n",
    "vocabulary = set(corpus_forLDA[0])\n",
    "for doc in range(len(corpus_forLDA)):\n",
    "    vocabulary = vocabulary.union(set(corpus_forLDA[doc]))\n",
    "vocabulary = list(vocabulary)\n",
    "# Set aspects of the corpus: D is n.doc, N is n.words, k is n.topics, V is len.vocabulary, w is a \n",
    "D = len(corpus_forLDA)\n",
    "k = 50\n",
    "V = len(vocabulary)\n",
    "N = len(corpus_forLDA[0])\n",
    "w = np.array([])\n",
    "for doc in range(D):\n",
    "    w = np.append(w, corpus_forLDA[doc])\n",
    "    \n",
    "w = np.reshape(w, (D, N))   \n",
    "set(corpus_forLDA[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-42e0c910bf6e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mbeta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtopic\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[0mbeta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirichlet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[0mbeta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mV\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.dirichlet (numpy/random/mtrand/mtrand.c:21135)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: float division"
     ]
    }
   ],
   "source": [
    "# Determine initial allocation\n",
    "from collections import Counter\n",
    "\n",
    "# Set the hyperparameters\n",
    "alpha = np.array([50/k] * k)\n",
    "eta = [200/V] * V\n",
    "\n",
    "# Initialise the document-specific mixing probabilities, D x k\n",
    "theta = np.array([])\n",
    "for doc in range(D):\n",
    "    theta = np.append(theta, np.random.dirichlet(alpha))\n",
    "theta = np.reshape(theta, (D, k))\n",
    "\n",
    "# Initialise the topic allocations, D x k - that is, in document d topic k occured this many times\n",
    "z = np.array([])\n",
    "for doc in range(D):\n",
    "    z = np.append(z, np.random.multinomial(N, pvals = theta[doc], size = 1))\n",
    "    \n",
    "z = np.reshape(z, (D, k))\n",
    "\n",
    "# Initialise the topic-specific term probabilities\n",
    "beta = np.array([])\n",
    "for topic in range(k):\n",
    "    beta = np.append(beta, np.random.dirichlet(eta))\n",
    "beta = np.reshape(beta, (k, V))\n",
    "\n",
    "# Draw the word allocations\n",
    "#w = np.array([])\n",
    "# Draw the word allocations\n",
    "#for doc in range(D):\n",
    "#    for zk in range(len(z[doc])):\n",
    "#            w = np.append(w, np.random.choice(vocabulary, p = beta[zk], size = z[doc, zk]))\n",
    "#    w = np.reshape(w, (D, N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:54: RuntimeWarning: invalid value encountered in divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-2fc0e21475bb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     41\u001b[0m             \u001b[0mbe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbeta\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtheta\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m             \u001b[0mdenominator\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mbe\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m             \u001b[0mbeta_theta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta_theta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;31m# We get this into a kxN matrix, because reasons\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/dist-packages/numpy/lib/function_base.pyc\u001b[0m in \u001b[0;36mappend\u001b[1;34m(arr, values, axis)\u001b[0m\n\u001b[0;32m   3553\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3554\u001b[0m         \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3555\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Start iterating\n",
    "it = 1\n",
    "total_results = []\n",
    "beta_collect = []\n",
    "while it < 20:\n",
    "    print it\n",
    "    \n",
    "    # Draw the new theta values\n",
    "    alpha_new = z + alpha\n",
    "    theta = np.array([])\n",
    "    for doc in range(D):\n",
    "        theta = np.append(theta, np.random.dirichlet(alpha_new[doc]))\n",
    "    theta = np.reshape(theta, (D, k))\n",
    "       \n",
    "    # Draw the new beta values\n",
    "    # Find the ms\n",
    "    count_vocab = dict(zip(vocabulary, [0] * V)) \n",
    "    count_terms = Counter(w[0])\n",
    "    for key in count_vocab:\n",
    "        count_vocab[key] += count_terms[key]\n",
    "    m = np.array(count_vocab.values())\n",
    "    eta_new = m + eta\n",
    "    # Find the betas\n",
    "    beta = np.array([])\n",
    "    for topic in range(k):\n",
    "        beta = np.append(beta, np.random.dirichlet(eta_new))\n",
    "    beta = np.reshape(beta, (k, V))\n",
    "    beta_collect.append(beta)\n",
    "    \n",
    "    # Draw the new zs\n",
    "    z_new = np.array([])\n",
    "    for doc in range(D):\n",
    "        # Our first task is to find the document-specific mixing probabilities\n",
    "        beta_theta2 = np.matrix(beta)\n",
    "        beta_theta2 = np.transpose(beta_theta2)\n",
    "        denominators = sum(beta_theta2)\n",
    "        # First we find a N x k matrix of beta[k,n] *theta[d,k] \n",
    "        beta_theta = np.array([])\n",
    "        denominator = [0] * V\n",
    "        for row in range(len(beta)):\n",
    "            be = beta[row] * theta[doc, row]\n",
    "            denominator += be\n",
    "            beta_theta = np.append(beta_theta, be)\n",
    "\n",
    "        # We get this into a kxN matrix, because reasons\n",
    "        beta_theta = np.reshape(beta_theta, (k, V))\n",
    "        beta_theta = np.matrix(beta_theta)\n",
    "        beta_theta = np.transpose(beta_theta)\n",
    "\n",
    "        # And then we simulate the z values\n",
    "        z_thisdoc = np.array([0] * k)\n",
    "        for word in range(len(w[doc])):\n",
    "            which_vocab = vocabulary.index(w[doc,word])\n",
    "            this_mix = beta_theta[which_vocab] / denominator[which_vocab]\n",
    "            z_thisdoc = z_thisdoc + np.random.multinomial(1, pvals = this_mix.getA1())\n",
    "        z_new = np.append(z_new, z_thisdoc)\n",
    "    z_new = np.reshape(z_new, (D, k))\n",
    "    z = z_new.copy()\n",
    "    \n",
    "    # Every 50th iteration, we take a sample\n",
    "    if it > 1000 & it % 50 == 0:\n",
    "        results = []\n",
    "        results.append(z)\n",
    "        results.append(beta)\n",
    "        results.append(theta)\n",
    "        total_results.append(results)\n",
    "    \n",
    "    it += 1\n",
    "\n",
    "print total_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.025375877833228876"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[1]\n",
    "max(results[1][1])\n",
    "consider = results[1][1].tolist()\n",
    "\n",
    "consider.index(max(consider))\n",
    "consider[1845]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6233\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[u'tripolitan', u'affairsw', u'yellow', u'interchang', u'elvi']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vocab = dict(zip(vocabulary, [0] * V)) \n",
    "count_terms = Counter(w[0])\n",
    "for key in count_vocab:\n",
    "    count_vocab[key] += count_terms[key]\n",
    "m = np.array(count_vocab.values())\n",
    "eta_new = m + eta\n",
    "print len(eta)\n",
    "vocabulary[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "6233\n",
      "1\n",
      "12466\n",
      "2\n",
      "18699\n",
      "3\n",
      "24932\n",
      "4\n",
      "31165\n",
      "5\n",
      "37398\n",
      "6\n",
      "43631\n",
      "7\n",
      "49864\n",
      "8\n",
      "56097\n",
      "9\n",
      "62330\n",
      "10\n",
      "68563\n",
      "11\n",
      "74796\n",
      "12\n",
      "81029\n",
      "13\n",
      "87262\n",
      "14\n",
      "93495\n",
      "15\n",
      "99728\n",
      "16\n",
      "105961\n",
      "17\n",
      "112194\n",
      "18\n",
      "118427\n",
      "19\n",
      "124660\n",
      "20\n",
      "130893\n",
      "21\n",
      "137126\n",
      "22\n",
      "143359\n",
      "23\n",
      "149592\n",
      "24\n",
      "155825\n",
      "25\n",
      "162058\n",
      "26\n",
      "168291\n",
      "27\n",
      "174524\n",
      "28\n",
      "180757\n",
      "29\n",
      "186990\n",
      "30\n",
      "193223\n",
      "31\n",
      "199456\n",
      "32\n",
      "205689\n",
      "33\n",
      "211922\n",
      "34\n",
      "218155\n",
      "35\n",
      "224388\n",
      "36\n",
      "230621\n",
      "37\n",
      "236854\n",
      "38\n",
      "243087\n",
      "39\n",
      "249320\n",
      "40\n",
      "255553\n",
      "41\n",
      "261786\n",
      "42\n",
      "268019\n",
      "43\n",
      "274252\n",
      "44\n",
      "280485\n",
      "45\n",
      "286718\n",
      "46\n",
      "292951\n",
      "47\n",
      "299184\n",
      "48\n",
      "305417\n",
      "49\n",
      "311650\n"
     ]
    }
   ],
   "source": [
    "beta = np.array([])\n",
    "for topic in range(k):\n",
    "    print topic\n",
    "    beta = np.append(beta, np.random.dirichlet(eta_new))\n",
    "    print len(beta)\n",
    "beta = np.reshape(beta, (k, V))\n",
    "beta_collect.append(beta)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
