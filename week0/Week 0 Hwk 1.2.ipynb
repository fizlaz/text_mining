{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Taking Rish's section of necessary modules to import\n",
    "# for establishing a connection between python and the targeted website\n",
    "import urllib\n",
    "\n",
    "# for retreiving information from websites \n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#convert xml as string to an ordered dictionary \n",
    "import xmltodict\n",
    "\n",
    "# to read arguments from command line\n",
    "import os \n",
    "\n",
    "#To use numpy arrays which are more efficient than regular data structures\n",
    "import numpy as np\n",
    "\n",
    "#To read the folder structure of the target\n",
    "import sys\n",
    "\n",
    "#to add randomised waiting time\n",
    "import time, random\n",
    "\n",
    "# to deal with regular expressions\n",
    "import re\n",
    "\n",
    "def get_headlines(url): \n",
    "    # Open the URL\n",
    "    html = urllib.urlopen(url).read()\n",
    "    soup = BeautifulSoup(html)\n",
    "    \n",
    "    # Find \n",
    "    div_list = soup.find_all(name = \"div\")\n",
    "    ## Create a regular expression for Python to search for\n",
    "    top_stories = re.compile(\"container-top-stories#[0-9]\")\n",
    "\n",
    "    headline_dict = {}\n",
    "    for div in div_list:\n",
    "        try:\n",
    "            if top_stories.search(div['data-entityid']) != None:\n",
    "                headline_dict[div['data-entityid'] ] = div \n",
    "        except:\n",
    "            None\n",
    "            \n",
    "    headlines = []\n",
    "    for key in headline_dict:\n",
    "        headlines.append(headline_dict[key].findAll(name = 'span')[0])\n",
    "\n",
    "    headlines_text = []\n",
    "    for headline in headlines:\n",
    "        for ch in headline.children:\n",
    "            headlines_text.append(ch)\n",
    "    \n",
    "    return headlines_text\n",
    "\n",
    "def scrape_headlines(urls, file_prefix):\n",
    "    headlines = []\n",
    "    \n",
    "    for url in urls:\n",
    "        try:\n",
    "            headlines.append(get_headlines(url))\n",
    "            with open((\"./data/projects/completed_urls.txt\", \"a\") as complete_file:\n",
    "                complete_file.write(url + '\\n')\n",
    "                complete_file.close())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'London to ban drones during Obama visit',\n",
       " u'Handshake family naturalisation halted',\n",
       " u'Champion Swiss snowboarder killed at 21',\n",
       " u\"Egypt tea vendor 'shot by police'\",\n",
       " u'New York vote key for Clinton and Trump',\n",
       " u\"Strikes on Syria markets 'kill dozens'\",\n",
       " u'Israeli guilty of murdering Palestinian',\n",
       " u'Kabul hit by large suicide attack',\n",
       " u'Sport',\n",
       " u\"EU should 'interfere' less - Juncker\",\n",
       " u\"Palmyra's arch recreated in London\",\n",
       " u\"Putin 'agreed formula' for prison swap\"]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_headlines(\"http://www.bbc.com/news\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
